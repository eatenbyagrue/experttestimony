%\documentclass[11pt, a4paper]{article}
\documentclass[11pt, a4paper]{scrartcl}

%\usepackage[a4paper,lmargin={3cm},rmargin={3.5cm}, tmargin={2.5cm},bmargin = {2.5cm}]{geometry}
\usepackage{setspace}
\usepackage{indentfirst}
\usepackage{enumitem}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage[backend=biber, authordate, ibidtracker=context]{biblatex-chicago}
\usepackage{fontspec}
\usepackage{titlesec}
\usepackage{color}
\usepackage{booktabs}

\newcommand{\mh}[1]{\noindent\emph{#1}}
\newcommand{\Ss}{S_{\sigma}}
\newcommand{\Ssa}{S_{\sigma\alpha}}
\newcommand{\Stsa}{S^t_{\sigma\alpha}}
\newcommand{\sa}{{\sigma\alpha}}
\newcommand{\given}[1][]{\:#1\vert\:}
%\newcommand{\Sm}{\Stsa{}m^t_{\sa}}
\newcommand{\Sm}{\Ss{}m_\sigma}
\newcommand{\CI}{\mathrel{\perp\mspace{-10mu}\perp}}
\newcommand{\nCI}{\centernot{\CI}}
\newcommand{\Var}{\mathrm{Var}}

\renewcommand{\Pr}{\text{Pr}}
\renewcommand{\i}[1]{\emph{#1}}
\renewcommand{\a}{\alpha}
\renewcommand{\b}[1]{{\osfamily{}#1}}

\newfontfamily\osfamily{Latin Modern Roman Demi}
\setkomafont{disposition}{\osfamily}
\setkomafont{descriptionlabel}{\osfamily}

\onehalfspacing{}
\addbibresource{testimony.bib}
\titleformat{\section}{\Large\bfseries\osfamily}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries\osfamily}{\thesubsection}{1em}{}


\title{\textbf{Bayesian Updating on Testimony} }
\subtitle{And How to Deal With Expert Testimony: The Preemption View}
\author{Conrad Friedrich \\ \texttt{conradfriedrich@posteo.net}}
\publishers{Formal Epistemology \\ Supervising: Reuben Stern, PhD \\ MCMP @ LMU Munich \\ }
\begin{document}

\maketitle
\thispagestyle{empty}
\tableofcontents
\newpage
\section{Introduction}

How to update one's belief when hearing an expert's testimony? This paper analyses a strategy called the Preemption View of authority testimony that deals with the special case of novice/expert testimony. To this end, a couple of examples are discussed and a broader survey of testimony in the Bayesian framework is developed. It is then possible to interpret the Preemption View using degrees of belief. 

This paper starts by exploring the non-Bayesian reasoning involved in the preemption account, following closely the arguments put forward by \textcite{Constantin2017}. They develop this argument mainly for flat-out belief. It becomes clear very quickly that to say much about how this affects credences, a more precise account is required. Since there are several options with regards to the choice of the framework, some of these are discussed. In particular, how should the trust an agent has towards a testifier be modeled? And in what form do they communicate? A look at what has already been developed in the literatur helps. Three different interpretations of testimony in a Bayesian framework by \textcite{Goldman1999-GOLKIA}, \textcite{Bovens2003} and \textcite{Olsson2013} resp. \textcite{Angere2010} seem interesting and relevant, and their usefulness for the purposes of analysing expert testimony is discussed. Settling the framework question, the central argument of \textcite{Constantin2017} is interpreted and translated into a context of degree of belief. For this, the notion of undercutting defeat gets some analysis, and some proposals are made to give a proper representation of the Preemption View in precise terms. 

A few remarks to boot. It's assumed that testifiers are honestly communicating their state of mind, that is, the veracity of the agents is not a factor in these considerations. While interesting, it does not shed additional light on this paper's question. Aside from that, the framework models both speaker and hearer, as some assumptions have to be made on both sides for communication to take place. 

\subsection{Expert Testimony}
Your doctor examines the red spots on your face and diagnoses: You have the measles! This gives you an excellent reason to believe that you actually have the measles. On the contrary, your accountant concludes that you have lupus does not give you a good reason to believe so.\footnote{This example is from \textcite{Constantin2017}.}

What's the difference? Of course, your doctor knows what she is talking about. She is an \i{expert} in diagnosing patients. What is the reasonable doxastic response to an expert's testimony? This paper examines two different general types of response to a case dubbed Novice/Expert problem \parencite{Goldman2001} for those cases in which the expert is an epistemic authority for the novice, which will be defined below. The first type of doxastic response --- how to update your beliefs in the face of new evidence, in this case expert testimony --- is usually attributed to Thomas Kelly \parencite{Kelly2010-KELPDA} and called the \i{Total Evidence View}. In a rather simplistic rendition relevant to this paper, the view might be summarized as follows: The expert's testimony is added to your stock of evidence for a proposition$p$. It may have a lot of evidential weight compared to the rest of your evidence, since you are competent enough to recognize the expert as an expert. Your own evidence and considerations, however, still bear on the question whether $p$, and are aggregated with the authority's testimony. So far, so standard --- it does not seem like anything else than very standard belief updating. Consequently, the precisification of this approach will just imitate the orthodox Bayesian conditionalization. \textcite{Constantin2017}, following \textcite{Zagzebski2012-ZAGEAA} and \textcite{Keren2014-KERTAB}, propose a different strategy called the \i{Preemption View} based on a concept of epistemic authority, which will be discussed next.

\subsection{Epistemic Authority and the Preemption View}\label{sec:undercut}

In short, the Preemption View states that in case of authority testimony, you should rationally believe like the authority does, disregarding your own evidence in that matter. In other words, the evidential weight of your own evidence gets reduced to zero in this consideration.

Of course, its crucial to exactly specify what is meant by \i{authority} and where the rationality of the view stems from. What makes it rational to deviate from more standard belief updating?

Let's unpack how \textcite{Constantin2017} develop this. Picture a simple testimony situation. First, the testifier has to be judged to be an expert by the agent whose doxastic attitudes are examined. They define, after \textcite{Goldman2001}, someone to be an expert in a certain area of expertise as (a) having a substantial body of evidence and (b) possessing and employing generally truth-conducive methods. Compared to a novice, an expert is an \i{epistemic superior} in her domain of expertise, someone who is more likely to be correct in matters of that domain. Epistemic superiority includes competence in evaluating evidence as well as access to substantive evidence. In particular, if the novice has second-order evidence that the expert \i{lacks} evidence which the novice herself possesses, the novice is licensed to withdraw her judgment that the expert is an epistemic superior in this case. 

Now, for an expert $A$ to be an \b{Epistemic Authority} for a novice $N$ in a domain $D$, $N$ has to reasonably believe that 
\begin{enumerate}[label = (\roman*)]
    \item $A$ is an expert about $D$, and
    \item $A$ is epistemically superior to $N$ with respect to $D$.
\end{enumerate}
A domain is a set of propositions best accessible by methods the expert has at her disposal. Note that while it is likely that $A$ is \i{actually} $N$'s epistemic superior, it doesn't necessarily have to the case, since what is crucial here is $N$'s reasonable belief, which can plausibly non-veridic. 

With these preliminaries in place, we can now reconstruct the central claim of \textcite{Constantin2017}. 
\begin{description}
    \item[Preemption View.] An authority's $A$'s testimony that $p$ in $A$'s expertise preempts all evidence of novice $N$ for or against $p$. Evidence is preempted with respect to $p$ if and only the evidence is rationally unusable for $N$ to assess $p$. 
\end{description}

In other words, the evidence is bracketed, so to speak, and loses its evidential force for $p$. It does not lose its evidential weight with respect to all other propositions, though.\footnote{As long as it Preemption View is complemented with another principle of rationality that requires something like consistency, this is not much of a problem. Otherwise it would be easy to find examples where evidential support against $p$ is preempted, but support against an obvious logical consequence of $p$, $q$, isn't, potentially leading to inconsistent belief sets.} 

Let's illustrate with an example from \textcite{Constantin2017}. 

\begin{singlespacing}
\begin{quote}
    Based on observations of everyday occurrences of pairs of events, it appears obvious to a layperson that two events are either simultaneous or they are not. Now she hears that widely respected physicists who specialize in relativity theory believe that this is not true [Such that it's not a fact of the matter whether two events are simultaneous, c.f.]. In this case, it seems completely irrational for the layperson to give any epistemic weight to her own everyday experiences in the face of the verdict of clearly identified experts. She should just follow the authority's lead. 
\end{quote}
\end{singlespacing}

Constantin and Grundmann take this as a case clearly speaking in favor of the Preemption View, based on pre-theoretic judgments and intuitions. Note how they stress that it would be \i{completely} irrational to give \i{any} evidential weight, which is a point that, as we'll see, does not fit very well with degrees of belief. They argue for it by presenting the principle as a special case of another normative principle widely accepted. Specifically, preemption works off undercutting defeat. Loosely speaking, some information $d$ is an undercutting defeat for some evidence $e$ towards some proposition $p$ if it removes or weakens the evidential of $e$ for $p$. To give an oft-quoted example of \textcite{Pollock1986}: In a factory, a person looks at an assembly line of widgets that appear red. Being appeared to red-widgetly, the person believes that the widgets are red. But a worker informs the person that there's a red light shining on the widgets, defeating her belief in the color of widgets, making it irrational to hold in the light of the defeater. Preemption, then, is a result from this type of defeat: The authority's testimony undercuts the rationality of trusting your own evidence. Here lies the argument for Constantin and Grundmann's normative claim. They claim that, given authority testimony, you can't base your belief on the result of your own reasoning processes anymore. Well, you \i{can}, but only on pain of irrationality. Perhaps counter to intuition, this also applies to cases where the authority agrees with your opinion. In the case of flat-out belief, this does not do much in terms of doxastic attitudes, as soon as you talk about credences, however, this has significant consequences. How to update your credence now?   

To appreciate the argument in this paper, which revolves all around the concept of undercutting defeat, it's expedient to have a look at what that's actually is. Constantin and Grundmann define it in the following way, following \textcite{Pollock1976-POLKAJ-4}:
\begin{quote}

Matters are different in the case of undercutting defeaters: $d$ is an  undercutting defeater for a subject $S$’s belief that $p$ on the basis of evidence $E$, iff $d$ is a prima facie reason that $S$ has to believe that relying on $E$ alone in assessing $p$ would not support the likelihood of $S$ being right about $p$ such that assessing $p$ on this basis would not be different from pure guessing. 

\end{quote}

This is quite a bit to unravel. The use of a counterfactual make the treatment a bit cumbersome, a point that we'll meet again when trying to find the right probability function (Section~\ref{sec:undercut}). But generally: Suppose you don't have any evidence for or against $p$. That's is the situation learning the defeater $d$ puts you in. 

That this happens epistemically when receiving authority testimony is the source of Constantin and Grundmann's normative claim. They argue for it by appeal to intuition on thought experiments, and I agree that there is definitely something to it. I do not necessarily join them in the normative force of complete irrationality of any evidential weight of the preempted evidence, however, as will be discussed below.

\subsection{The Need for Precisification}

While the account is promising and well-argued for in the case for flat-out, or ternary, belief, it is much less fine-tuned if you take a look at what this means for degrees of belief as a framework of doxastic states. Especially the main argument is worrisome: To my knowledge, there hasn't been a worked-out theory of undercutting defeat for degrees of belief so far. At least Constantin and Grundmann are not mentioning one. The result is that in the case of degree of belief, it is rather unclear what the conditions are for authorities to be recognized as such, or for reasons to be preempted. Worse, without a general principle of undercutting defeat to defer to, the normativity of the preemption principle stands questioned. This is not \i{necessarily} bad news for their account, as the problems do not arise for flat-out belief. But since they explicitly state to include degrees of belief in their argument, it is worth to have a closer look.  

Here is a particular worry with the argument put forward by Constantin and Grundmann applied to credences: They frequently appeal to both flat-out beliefs and credences when defining their central notions like experts, authorities and undercutting defeat. For example, following Goldman, experts are defined as possessing methods that reliably lead to true beliefs and adequate credences. In the case of true beliefs, this is easily understood. But what does that mean for a credence? One might think that adequate credence means something like the unique rational credence given a fixed body of evidence in the sense that the Uniqueness Thesis claims it exists.\footnote{cf. \textcite{Feldman2007-FELRRD}.} But then, one would have to presuppose Uniqueness just to define what an expert is. That seems excessive. Instead, and charitably, one could suppose a notion where an expert's testimony tested for reliability. This is, in theory, as familiar a notion as for flat-out belief, and the approach that's adopted in the present paper. The details and ramifications are still very much unclear, and to clarify these notions will be one of the central motivations for this paper.  

\subsection{Evaluation}

What kind of argument can the present paper deliver? In brief, it strives to be a normative one, of course, since the question which rational strategy to employ is a strictly normative one. Epistemic normativity for degrees of belief is of course hotly debated, but most agree that there is some sense to the notion. Usually, normativity is derived from the standard probability axioms plus conditionalization, which are taken as normative themselves. Or even these are justified through practical arguments like Dutch Books or purely epistemic arguments that derive their normative force from attributing epistemic value to doxastic attitudes which are measurably close to the truth. By showing that an epistemic practice generates accurate credences, these practices are endowed with normative force. What exactly the accuracy of a credence amounts to is defined decisively non-homogeneous, but usually credences are distinguished as more accurate that differ less in their probability assignment from what is taken to be the true value of a proposition. 

In this paper, both strategies are used, in that I take undercutting defeat to already be a justified principle (which of course can be contested, but isn't in this paper) and also evaluate the accuracy of credences generated by the strategies proposed. 

\section{Bayesian Updating on Testimony}

\textcite{Olsson2013} develop a very intriguing account of how testimony can be modeled in the Bayesian framework. They go on and actually implement the account in a computer simulation to address various questions regarding informational movement in a social network. Here, the focus is on a single or very few transactions between agents, and in contrast to their account time steps are not critically involved in the considerations, since there usually only is a before and after the update. Exceptions are noted as such. Note, too, that the indices relating the probability assignment to an agent are omitted, since usually only a single agent's credences are looked at.   

The presented account models the reliability of agents as well as agent's opinions about agent's reliability. This yields a much richer framework than simple Bayesian updating, and consequently a thorough description is in order.

To keep things manageable, there is a single \i{true} target proposition $p$ which the agents have a credence $\Pr_\alpha(p)$ towards. Agents can receive information from a \i{source}. These can be (i) other agents in the social network or (ii) their own inquiry. The objective chance that an agents inquires and gets it right is called that agent's \i{aptitude}: $P(S_{\iota}p \given S_{\iota} \land p)$, where $S_{\iota}$ is the proposition that the agent inquires for evidence, regardless of the results, and $S_{\iota} p$ the chance that she inquires and her inquiry yields $p$ as a result. 

Sources can be more or less reliable. The reliability is assumed to be symmetric. Here, $\sigma$ is a placeholder for a source to plug in. 
\[ 
R_{\sigma} =_{df.} P(\Ss p \given \Ss \land p) = P(\Ss \neg p \given \Ss \land \neg p)
\]
This is, of course, just the agent's aptitude. Note that the reliability of an agent is modeled continuously, in contrast to the binary full reliability/randomiser approach employed in \textcite[Chp. 3]{Bovens2003}, such that with a reliability value of less than $0.5$ an agents inquiry would actually be negatively correlated with the truth.

Agent trust sources to a certain extent. This is expressed in the agent's credence in the reliability of the source:
\[ 
    \Pr(a \leqslant R_{\sigma} \leqslant b) = \int_a^b \tau_{\sigma}(\rho) d\rho
\]
where $\tau_{\sigma}: [0,1] \rightarrow \mathbb{R}^+$ is a probability density function such that 
\[
    \Pr(0 \leqslant R_{\sigma} \leqslant 1) =  \int_0^1 \tau_{\sigma} (\rho) d\rho = 1
\]which is a very plausible requirement on a rational credence function.\footnote{In fact, not obeying this would violate the probability axioms. For some reason, \textcite{Angere2010} as well as \textcite{Olsson2013} define the trust function as $\tau_{\sigma}: [0,1] \rightarrow [0,1]$, such that it doesn't integrate to 1 (in all but the trivial case). I assume they are doing some hidden normalising that I am to near-sighted to see.}

\begin{description}
    \item[Example.] An agent with a healthy trust in her own inquisitive abilities and who is not easily swayed in this trust may have a trust function $\tau_{\iota}$ described by a beta distribution
\[
    \tau_{\iota} (\rho) = \frac{1}{B(a,b)} \rho^{a - 1} {(1 - \rho)}^{b-1}
\]
with normalising beta function $B(a,b)$ and parameters $a, b$ chosen such that $\tau^t_{\iota\alpha}$ is densest around whatever a `healthy trust' amounts to, let's say 0.85. Interestingly, the shape of the curve correlates with how resilient the function behaves upon updating. More on that below.
\end{description}
Apart from inquiries, a source can also be another agent, through testimony. An agent $\alpha$ therefore trusts herself at time $t$ with $\tau_{\iota}$ and another agent $\beta$ with $\tau_{\beta}$.

The exchange of information and the evolution of the trust functions and the credences in $p$ are the most important going-ons in this model, which I'll summarize next.

Testimony is, of course, a matter of language, and is most plausibly modeled as a binary judgment, that is, either $p$ or $\neg p$. While there are certainly interesting cases where the testimony involves credences or levels of confidences, e.g.\ a weather forecaster stating her belief in the chance of rain tomorrow, this does in my view not yield a general notion. The simple testimony is much more common. 

Updating the credences is based on new messages the agent receives from sources: $\Sm$, where $m^\sigma$ is the message that the agent receives received from source $\sigma$ (which can represent her own inquiry or another agent's testimony), and is either $p$ or $\neg p$. An agents can receive multiple messages at once.

The central updating rule is given by conditionalization:
\[
    \Pr(p) = \Pr (p \given \bigwedge_{\sigma \in \Sigma} \Sm),
\]

where $\Sigma$ is the set of sources that send a message to the agent.

With the assumption that sources are \i{independent} given the target proposition (this plays a major role later on), the update calculates to
\begin{equation}
    \label{eq:upd}
    \Pr (p \given \bigwedge \Sm) = \
     \frac{\Pr (p) \prod \Pr (\Sm \given p) }
    {\Pr(p) \prod \Pr (\Sm \given p) +  \Pr (\neg p) \prod \Pr (\Sm \given \neg p) }.
\end{equation}

Here, again, the product and big conjunct range over all sources with a message for the agent, the subscript omitted for legibility.   

The expressions about the reliability are given by:
\begin{align*}
    \Pr (\Ss p \given p)            &= \Pr (\Ss) \langle \tau^t_{\sa} \rangle \\
    \Pr (\Ss \neg p \given p)       &= \Pr (\Ss) \langle \bar{\tau}^t_{\sa} \rangle \\
    \Pr (\Ss p \given \neg p)       &= \Pr (\Ss) \langle \bar{\tau}^t_{\sa} \rangle \\
    \Pr (\Ss \neg p \given \neg p)  &= \Pr (\Ss) \langle \tau^t_{\sa} \rangle,
\end{align*}

where $\langle \tau_\sigma \rangle $ is the expected value of trust function $ \tau_\sigma $ and ${\langle \bar{\tau_\sigma} \rangle =_{df.} 1 - \langle \tau_\sigma \rangle}$.\footnote{Detailed and very helpful derivations of these equations can be found in \textcite{Angere2010}, although in places I could not agree with the results. I implemented the updating mechanism as presented in this paper. There is not much room to detail the finer points here, only this much: Angere's final result on page 22 for the credence update intuitively can't be right, since the expression as stated there does not depend on the actual content of the message, that is, $p$ or $\neg p$, anymore, and instead just updates regardless, rendering the network communication ineffective.} 

Where this account really shines is the possibility to update the trust function based on the evidence. Although not strictly needed for the present purposes, I demonstrate the updating procedure for the case that evidence regarding $p$ is received. This example alone opens up very interesting structural relations between the credences and the trust functions involved.

Upon receiving a message, the agent updates her trust function for that source depending on how well the message coheres with her own credence. The updated trust function can be calculated to
\[
    \tau_\sigma = \tau_\sigma (\rho) \frac{\rho \: \Pr(p) + (1 - \rho) \Pr(\neg p)}
    {\langle \tau_\sigma \rangle \Pr(p) + \langle \bar{\tau_\sigma} \rangle \Pr(\neg p)}
\]
if $m_\sigma = p$, and 
\[
    \tau_\sigma = \tau_\sigma (\rho) \frac{\rho \: \Pr(\neg p) + (1 - \rho) \Pr(p)}
    {\langle \bar{\tau_\sigma} \rangle \Pr(\neg p) + \langle \tau_\sigma \rangle \Pr(\neg p)}
\]

if $m_\sigma = \neg p$. 

So far for the general Bayesian strategy of belief updating in the Olsson and Vallinder model. In the next section, the whole apparatus is applied to the case at hand, authority testimony. 

\section{The Preemption View for Degrees of Belief}

\subsection{Undercutting Defeat With Numbers}

At the heart of Constantin and Grundmann's argument lies the notion of undercutting defeat. While they distinguish between several types of undercutting defeat, they recognize that the different types have a central property in common they ``screen off'' evidence from a hypothesis \parencite[fn. 18]{Constantin2017}. The notion is meant to be understood fairly intuitively, in the sense that the defeater reduces the evidential impact the evidence has on a rational attitude towards the hypothesis. As described above, the standard examples illustrate this point. The choice of words is telling when it comes to the interpretation of an undercutting defeater as screening off the relevant evidence, since it hints at the property of a random variable screening of another from a third given a subjective probability function. Let's look at a general case of evidence confirming or disconfirming a hypothesis. Somewhat more precise, here is how Mike Titelbaum describes it:

\begin{description}
    \item[Screening Off.] $D$ screens off $E$ from $H$ when $E$ is unconditionally relevant to $H$ but not relevant to $H$ conditional on either $D$ or $\neg D$.\footnote{cf.\ \textcite[270]{Titelbaum2017}}
\end{description}

This amounts to three conditions for an agent's credence function $\Pr$:\footnote{cf.\ \textcite[70]{Titelbaum2017}}
\begin{align*}
    \Pr(H \given E) &\not = \Pr(H)\\
    \Pr(H \given E \land D) &= \Pr(H \given D)\\
    \Pr(H \given E \land \neg D) &= \Pr(H \given \neg D)
\end{align*} Or, in other words, again: $E$ is evidence for or against $H$. But conditioning on $D$ as well, $D$ screens off all evidential impact that $E$ has on $H$, and the same goes for $\neg D$. Titelbaum specifically requires the screening off on the negation, too, but that is not crucial for the present purposes.

Some parallels with the concept of undercutting defeat may already be visible, as the letters for the propositions have been chosen rather ominously. It is still a good idea to look at approaches from the literature, where researching scholar isn't particularly spoiled with papers on this topic. Matthes Kotzen's approach can be summarized for the present purposes like this:  
\begin{description}
    \item[Undercutting Defeat, Kotzen.]$D$ is an undercutting defeater for the evidence that $E$ provides for $H$ (relative to background information $K$) just in case \linebreak ${\mathfrak{c}(E, H, K) > \mathfrak{c}(E. H, K \land D)}$.\footnote{cf. \textcite[13]{Kotzen2010}}
\end{description}
The function $\mathfrak{c}$ is here a placeholder for a confirmation measure which remains to be selected, but in any case of course makes use of a probability function.\footnote{For a discussion and overview cf. \textcite{Fitelson}.} What does Kotzen's definition say? Basically that the degree to which evidence confirms the hypothesis is reduced when adding a defeater to one's stock of beliefs. 

Note that there are differences betweens Kotzen's proposal and mere screening off, which is a lot stronger than Kotzen requires. Screening off prescribes absolutely no evidential weight left after conditioning on the defeater, while Kotzen merely needs a decrease in evidential support. First assume that $E$ is evidence for $H$, such that $\mathfrak{c}(E,H,K) > 0$. When the measure function is, for example, assumed to be defined naively as 

\[\mathfrak{c}(E, H, K) =_{df.} \Pr(H \given E \land K) - \Pr(H \given K) ,\]

then the undercutting defeat condition requires that  
\begin{equation}\label{eq:undercut}
\Pr(H \given E \land K) - \Pr(H \given K)  >  \Pr(H \given E \land K \land D) - \Pr(H \given K \land D)  \geqslant 0.
\end{equation}

Which is just to say that for $D$ to be an undercutting defeater for $E$ regarding $H$, conditioning on $D$ lowers $E$'s degree of confirmation for $H$. Note that the requirement states that the second term $ \Pr(H \given E \land K \land D) - \Pr(H \given K \land D) $ is greater \i{or equal} to 0. If equal, the requirement of screening off is fulfilled, since then $\Pr(H \given E \land K \land D) = \Pr(H \given K \land D)$ (We may ignore the background knowledge $K$ to make it fit the description of screening off above).

So there seems to be an interesting connection between the notion of undercutting defeat and screening off: In the most extreme case, an undercutting defeater screens off the evidence it defeats from the hypothesis. Such that screening off is a sufficient, but not a necessary condition for undercutting defeat.

By constituting an extreme case, screening off may be thought of as what in the case of flat-out belief is called a \i{full undercutting defeater}. No evidential import is left, the evidence is fully undercut. Constantin and Grundmann contrast this with another concept called \i{partial undercutting defeater}. A defeater can only have partial impact if it's support isn't rock solid or if it just does not have enough evidential weight to completely deprive the original evidence from its import. This of course neatly fits the account described here: An undercutting defeater is usually partial and becomes a full undercutting defeater only in extreme cases. One has to add, that the account Constantin and Grundmann elicit very probably mean something slightly else here: Their focus is on whether a belief stays epistemically justified and, one might think, envision something like a threshold of justifiedness that is somehow tied to the degree of confirmation the evidence licenses for the hypothesis. If a defeater is strong enough such that the belief loses its justification given the evidence and defeater, the defeater is considered full. There might still be evidential import left, but not enough to render the belief justified. But since that is all flat-out belief talk and there is not a straightforward translation into degree of belief talk,\footnote{Although, of course, there are attempts, cf.\ e.g.\ \textcite{Pettigrew2017}} it might do for the present purposes to got with the above distinction between partial and full undercutting defeat. 

How does this relate to another central notion, the \i{rebutting} or \i{opposing} defeater? First a look at what \textcite[11]{Constantin2017} define:  

\begin{quote}
Roughly, $d$ is a  rebutting defeater for a subject S’s belief that $p$ on the basis of evidence $E$, iff $d$ is a prima facie reason that $S$ has to believe that $\neg p$, such that  the resulting total evidence  $E+d$ is neither equivalent to $E$, nor to $d$. 
\end{quote}

In other words, the rebutting defeater is just any old evidence for the negation of the hypothesis. That is, additionally to the requirement of undercutting, the rebutting defeater needs to to provide positive evidence for the negation of the hypothesis. Kotzen defines:

\begin{description}
    \item[Rebutting Defeat, Kotzen.] $D$ is a rebutting defeater for the evidence that $E$ provides for $H$ just in case: $\Pr(H \given E) > \Pr(H \given E \land D)$ and $\Pr(H \given D) > \Pr(H)$.
\end{description}

How does the rebutting defeater also undercut? Suppressing the background knowledge $K$ for the moment, we have directly 
\[{ \Pr(H \given E) - \Pr(H)  >  \Pr(H \given E \land D) - \Pr(H\given D) \geqslant 0 }\]
This result should make intuitive sense: A rebutting defeater, meaning a reason against the hypothesis, reduces the evidential import of the evidence if added to one's total evidence.     

Some things have to be clarified with regards to the probability function in question. It is not really clear from what was said so far how we can recover the probabilities where we did not condition on the evidence already, because, let's assume, the agent reached her current probability function through classic conditionalization. Then her subjective probabilities of course already include all the available evidence acquired at this point. But to talk about defeaters like above, her probability assignment \i{without} conditioning on the evidence is highly relevant. The agent needs to somehow forget what she learned with certainty already. Since this is not intended in the Bayesian paradigm, concept like hypothetical priors is introduced. Hypothetical priors are such that they represent an agent's credences before she learned anything empirical.\footnote{cf. \textcite[Ch. 4]{Levi1980}, \textcite[110]{Titelbaum2017}} Then, assuming she updated her beliefs always and only in accord with conditionalization, an agent's credences at a time $t$ are represented by a probability function that results from conditioning her hypothetical priors on her total evidence at that time.

That is, let $E_t$ be the agent's total evidence at time $t$. Then her probability function at $t$ results on conditioning her hypothetical priors on the evidence up to this point such that $\Pr_t(\cdot) = \Pr_H(\cdot \given E_t)$. Now the connection to undercutting defeat seems to be this: Suppose the agent learns of the potential defeater at time $t$. To determine whether equation~\ref{eq:undercut} is satisfied, the probability function to represent her credences has to be chosen. The correct function $\Pr_c$\footnote{The $c$ is for `correct'!} seems to be determined by her hypothetical priors conditioned on her available evidence at that time \i{without} all evidence to be defeated and the defeater. Formally: $ \Pr_c(\cdot) = \Pr_H (\cdot \given E_t \setminus (E \land D))$. Then this function $\Pr_c$ can be tested against equation~\ref{eq:undercut}.

\subsection{Authority Testimony in Olsson and Vallinder's Model}

The interesting question now is how to these potentially accurate precisifications fare when applied to the case at hand: the Preemption View interpretation of authority testimony. The interplay of undercutting defeat and authority testimony has been described at length in Section~\ref{sec:undercut}. The setup, quickly summarized: Constantin and Grundmann claim that an expert's testimony that $p$, who you recognize as more reliable than you in this domain, preempts your evidence for $p$ and constitutes an undercutting defeater. Even if you agree withe expert! Additionally, the testimony gives you excellent reason to believe that $p$ is the case.  
With the preliminaries this in place, can all this talk of undercutting defeat and authority testimony be followed by a proper integration into a Bayesian Framework? After all, that's what we set out to do. Since this is not straightforward, in the following, I develop different approaches and discuss them in turn.

The following sections finally try to answer the question: How to update one's credences given expert testimony, or formally, 
\[
    \Pr(p \given \Sm) =\,?
\]
\subsubsection{The Thresholding Method}

The approach that seems to be the most obvious is as closely modeled after Constantin and Grundmann as possible. The gist is this: Whether you accept an expert as an authority is either true or false, and is completely determined by the following principle: 

\begin{description} 
    \item[Epistemic Authority] An agent $\alpha$ recognizes another agent $\beta$ as an epistemic authority regarding proposition $p$ at time $t$ if and only if  
    \begin{enumerate}[label= (\roman*)]
        \item $\langle \tau^t_{\beta\alpha} \rangle \geqslant T_A$, where $T_A$ is a reasonably high threshold of trust, and
        \item $\langle \tau^t_{\beta\alpha} \rangle - \langle \tau^t_{\iota\alpha} \rangle \geqslant \Delta_A$, where $\Delta_A$ denotes some difference in trust level.
    \end{enumerate}
\end{description}

In plain English: Consider a layperson and an expert. For the layperson to regard the expert as an epistemic authority, she has to (i) regard the expert with a high amount of trust. The threshold is high, somewhere above $.8$. Condition (ii) requires that the expert enjoys a higher trust from the layperson than the layperson trusts her own abilities to evaluate evidence. This might also be modeled as a ratio, but in this case is not crucial.

The advantage of the Olsson Vallinder model becomes self-evident here: The trust level that decides over authority acceptance is endogenous to the model. Somewhat similar to the Lockean Thesis concerning the relation of full belief and partial belief, this approach connects recognizing an authority to a level of trust in that person by a threshold measure. If that threshold is crossed, and the trust level is higher than the agent's own, the authority is recognized as such. 

How does that help with the updating rule? We now have an exact criterion, when an agents recognizes an authority. Now let's add two further assumptions. First, the  authorities testimony generates a \i{full} undercutting defeater for all of the agent's relevant evidence. Second, where there is no evidence, one should be indifferent --- the principle of indifference.\footnote{Which is not too bold an assumption, I presume. Also, cf. \textcite[Ch. 5.3]{Titelbaum2017} for a helpful discussion and its relation to the maximum entropy principle.}   

The full undercutting defeater of the authority testimony has the effect that it screens of all relevant first-order evidence such that $\Pr(p \given E \land \Sm) = \Pr(p \given \Sm)$. For simplification the indices for hypothetical priors are omitted.

Now, if this is the case, the agent's credence in $p$ should be $\Pr(p) = \Pr(\neg p) = 0.5$. As described above, the new credence then calculates with 

\[
    \Pr(p \given \Sm) = \frac{\Pr(p) \langle \tau_{\beta\a} \rangle}
    {\Pr_\a(p) \langle \tau_{\beta\a} \rangle + \Pr(\neg p) \langle \bar{\tau}_{\beta\a} \rangle},
\]
then simple algebra gives 
\[
    \Pr(p \given \Sm ) = \langle \tau_{\beta\a} \rangle.
\]

Which seems like a rather promising result, does it not? Intuitively, it makes sense to adopt the credence corresponding to the trust level of the authority. Since before, there is a hunch of arbitrariness associated with  choosing a threshold above which to recognize an expert as an authority, this seems to unfairly favor trust levels close above the threshold over higher trust levels. Once you're above the threshold, it is a full undercutting defeater, and additional trust does not factor in. In this account, however, the trust level directly factors into the credence again, giving proper weight to the intuitive import of trust. The threshold still is kind of arbitrary and would have to be reasoned for independently, just as the notion that over the threshold, the testimony constitutes a full defeater, full stop.

Is this the solution, then? Perhaps not quite: there is some evidence speaking against it. In another paper, I tested this method in an agent-based computer simulation, calculating the outcomes of comparable agents employing this strategy vs.\ the more standard Bayesian strategy proposed by Olsson and Vallinder, while varying the starting parameters. The outcomes were evaluated by their accuracy after ten time steps of activity in the social network, presupposing a veritist notion of epistemic normativity, as Olsson and Vallinder do in their experiments, too. Since veritism isn't too uncommon in formal epistemology, I take it that the approach has at least some normative import. The results were quite clear: Given the experimental setup, the standard Bayesian strategy \i{clearly} trumps the preemptive strategy on accuracy. However, and that's with a capital \i{H}, there are some caveats to consider. Since the credence is based on expected trust, and expected trust is based on the trust function, and the updating of the trust function is heavily dependent on the shape of the trust function, the change in credence bigly depends on the initial shape of the trust function. The difference in accuracy of both strategy's credences can plausibly be explained by the lackluster speed with which the expected trust level changes. Also, the trust levels of the agent's in their own abilities where rather on the low end. A different, less pronounced shape of the initial trust function and change in self-trust could render this result moot, or at least not as grave. In brief, by varying even more parameters of the experiment the results could change significantly. So while the experiment provides some clues about the feasibility of the strategies, the issue is far from settled. 

\subsubsection{The It's-Just-Bayesian Method}

To tie on the remarkable accuracy of the standard conditionalizing, what if there is no special treatment of authority needed in the Bayesian framework, after all? Maybe there is such a thing as free lunch and the orthodox updating already suffices for the present purposes. Well, let's find out. 

The question is whether the authority's testimony constitutes an undercutting defeater like specified above. We test the updating method against Equation~\ref{eq:undercut}, reformulated here like so:

\[
    \Pr(p \given E) - \Pr(p) > \Pr(p \given E \land \Sm) - \Pr(p \given \Sm) \geqslant 0
\]

To find an answer, we need to determine what $E$ actually is in our model and how to recover the hypothetical priors for this probability function, since all we have is the \i{current} probability function after the agent learned all the evidence and before she learned of the defeater. 

Since this is not easily obtainable, the way proposed here might seem like a sleight of hand, but may be reconciled with some argument. Recall that the agent has a trust function for her own inquiries in matters of $p$, or from that domain, as well. It may not be completely off to interpret the agent's current credence in that model as a result of conditioning on her own inquiry. That is, here rational credence after her own evaluation equals her own expected trust in this matter. This may seem like a variant of Lewis' Principal Principle, in a way, but is this this model's answer to what a reflective person should assign her credences. You just treat yourself as another source for information.\footnote{This is, of course, somewhat contentious and raises a number of concerns and questions. A generally interesting issue, however, for the present purposes I take it as plausible enough.}   

If we additionally adopt the principle of indifference again, we have

\begin{table}[ht]
\centering
\begin{tabular}{@{}cccc@{}}
\toprule
$\Pr(p \given E)$ & $\Pr(p)$ & $\Pr(p \given E \land \Sm)$ & $\Pr(p \given \Sm)$ \\ \midrule
$\Pr(p \given S_{\iota}p )$ & 0.5 & $\Pr(p \given S_\iota p \land S_\beta p)$ & $\Pr(p \given S_\beta p)$
\end{tabular}
\end{table}

That $\beta$ is an authority for the agent requires $\langle \tau_\beta \rangle > \langle \tau_\iota \rangle$. Plugging this into the inequality for undercutting defeat, one can quickly see that in most plausible cases, the condition is fulfilled.

It seems like the standard Bayesian updating already respects our formal requirement for preemption, then. The evidence is not completely preempted, that much is true, but it seems like it doesn't really have any real impact on the result any more.  

But, there is a case with an intuitive judgment which I consider a knock-down argument. Suppose not one, but two competing authorities. So far this account hasn't talked about two authorities, but it's very straightforward to integrate two authorities in the orthodox Bayesian way. But first, check your intuitions. Two experts you regard as authorities of a field with about equal trust contradict each other. One says $p$, the other $\neg p$. Isn't the proper response here to be very cautious with a judgment, and most probably suspend? 

Conditioning on both opinions, as was just proposed, however, delivers the verdict that both testimonies cancel each other out, entitling you to give \i{full weight} to your own reasoning. That is, the rational response here would be to fully trust your own reasoning and ignore the authorities testimony. This can easily be seen: Recall Equation~\ref{eq:upd} on page~\pageref{eq:upd}. With two testimonies with $\langle \tau_\beta \rangle \approx \langle \tau_\gamma \rangle$ for authorities $\beta, \gamma$
\[ \Pr(p \given S_\beta p \land S_\gamma \neg p) \approx \Pr(p). \] Of course, now the condition for an undercutting defeater is not fulfilled. If one in any way warms up to the idea that a recognized expert diminishes the reasonability of relying on one's own judgment, this result can't be just accepted. It would spell a depressing result for public discourse for example, whenever experts disagree, just ignore them!  

\subsubsection{The Incomplete Method}

What, then, potentially alleviates the troublesome news? I think the answer lies in how the trust functions themselves are updated. Here is the rationale: The testimony of an authority diminishes the agent's trust in her own inquisitive abilities regarding the current proposition and propositions just like it. The degree of diminished trust is proportional to the trust the agent places in the authority, and anti-proportional to the trust an agent places in herself. Such that were there a host of experts all testifying, the expected value of trust in her own reasoning and inquisitive abilities in this matter tends to $0.5$. 

How can this idea be made to fit in the Olsson Vallinder model? First, we have to again use the sleight of hand from above and regard the prior credence as the result of a conditionalisation of an indifferent credence on one's own inquiry. Additionally, there is a tacit assumption explicit in \textcite{Angere2010} that is briefly mentioned on page~\pageref{eq:upd}: The source messages are assumed to be probabilistically independent. 
\[\Pr \left( \bigwedge S_{\sigma}m_\sigma \given m_\sigma\right) = \prod \Pr\left( S_{\sigma}m_\sigma \given m_\sigma\right)\] 
But this, as just laid out, does not hold here. Consider the case of two conflicting authority testimonies $S_\beta$ and $S_\gamma$. The testimony of the authorities lower the credence in the reliability of one's own reasoning. That is, $\Pr(S_\iota p \given p) \geqslant \Pr(S_\iota p \given p \land S_\beta m_\beta \land S_\gamma m_\gamma)$. This simple condition makes sure that the updated credence is always less or equal than if we would assume source independence, as can easily be checked by plugging it into Equation~\ref{eq:upd} on page~\pageref{eq:upd}. But how is the new likelihood calculated? Frankly, I haven't been able to figure it out yet do to time constraints, so this paper is very much still work in progress. I would love some input and discussion on it, though!

Apart from the missing derivation, this approach seems to be promising. It does justice to the notion of an undercutting defeater while still fit into a Bayesian framework. 

\section{Conclusion}


What, then, can be said to be the result of this paper? First, a general representation of the epistemic phenomenon of undercutting defeat has been presented. This notion has been used to argue for a particular strategy of belief updating when faced with expert testimony. While the more fleshed out proposals did have grave objections, the most promising proposal of giving up source independence still needs work. Nevertheless, I hope to have convincingly laid out Constantin and Grundmann's argument that an authority's testimony constitutes an undercutting defeater for your evidence. By tying together the Olsson and Vallinder model which includes trust as a central notion and the Preemption View, I hope to have shown that the proposed interpretation of undercutting defeat has some internal plausibility and is actually sensibly applicable. Of course, the scope of this paper may be a tad grand, and the  proposed solutions are only tiny steps.

The model, as it stands, may not be fine grained enough, though, as Constantin and Grundmann do make finer points towards the end of the paper by differentiating between different types of trusts in the same person in the same domain, but then it was never the goal to give a complete picture. 

The present argument could be refined to show that on the promising proposal, updating like described leads to a general increase in accuracy. This would help support the view, but has to wait for another paper.

Of course, the proposal as it stands is open to objections. For example: Is it really that clear that an authority's testimony undercuts your evidence, even if it generally agrees with your opinion? Shouldn't it instead \i{boost} your credence, if ever so slightly? This is, I think, a valid point and not so easily dispelled. 
If the mere information that an authority has a particular opinion already undercuts my evidence, doesn't already the belief that an authority has any opinion at all? And can't I reasonably expect authorities having opinions about many things, rendering all my evidence moot? There are ways to address this slippery slope, but they, too, are out of the scope of this paper.

\printbibliography{}
\end{document}

